# 句子切片与检索策略实验报告

1. 实验设置
- **模型**: Qwen-Plus
- **Embedding**: DashScope text-embedding-v3
- **数据**: 人工智能、机器学习、深度学习相关介绍文档

2. 不同切片策略效果对比

| 切片策略 | 参数设置 | 检索到的上下文特点 (观察) | 回答准确性/完整性 |
| :--- | :--- | :--- | :--- |
| **Sentence Splitter** | chunk_size=256, overlap=20 | (例如：上下文较连贯，但有时包含无关信息) | (例如：准确) |

[Generated Answer]:
人工智能历史上的低谷期（AI Winter）是指在人工智能发展过程中，由于技术瓶颈、预期过高未能实现以及资金和兴趣的减少，导致研究进展缓慢和资源投入下降的时期。


| **Token Splitter** | chunk_size=128, overlap=10 | 容易在句子中间截断，阅读体验差 | 受截断影响，有时信息不全 |
的诞生。在随后的几十年里，AI经历了多次繁荣与低谷（AI Winter）。
21世纪以来，随着大数据、云计算和算力的提升，AI迎来了爆发式增长。特别是深

| **Sentence Window** | window_size=3 | (例如：检索精准定位到单句，后处理扩展了丰富上下文) 

window: 人工智能（Artificial Intelligence，AI）是计算机科学的一个分支，致力于研究如何让计算机模拟人类的智能行为。它涵盖了机器学习、深度学习、自然语言处理、计算机视觉等多个领域。


[Generated Answer]:
人工智能历史上的低谷期（AI Winter）是指在人工智能发展过程中，由于技术瓶颈、预期过高未能实现以及资金和兴趣的减少，导致研究进展缓慢和投资下降的时期。这些时期通常伴随着对AI技术的失望和批评，使得相关研究项目难以获得支持。



3. 问题分析

Q1: 哪些参数显著影响效果？为什么？
我改了Sentence Splitter/Token Splitter/Sentence Window参数，chunk_size变大一倍，它丰富了答案的回答，提供了更多的上下文。
Sentence Window扩大几倍后，不仅能丰富上下文，并且还添加了人们对于ai能力的看法，回答既有客观原因也有主观因素

Q2: chunk_overlap 过大或过小的利弊？
过大: 当chunk_overlap大于chunk size会报错,Got a larger chunk overlap (300) than chunk size (256), should be smaller.
过小: 生成的答案会失去一些上下文的丰富性（chunk_overlap=1）
人工智能历史上的低谷期（AI Winter）是指在人工智能发展过程中，由于技术瓶颈、预期过高但实际进展有限，导致研究经费减少、学术兴趣下降的时期。这一时期通常伴随着对AI技术的失望和投资缩减，是AI发展历程中经历的多次繁荣与衰退周期中的“寒冬”阶段。

原本的设置，chunk_overlap=50
人工智能历史上的低谷期（AI Winter）是指在人工智能发展过程中，由于技术瓶颈、预期过高但实际进展有限，导致研究经费减少、学术兴趣下降的时期。这些时期通常出现在早期对AI的乐观预期未能实现之后，（使得政府和企业对AI投资大幅缩减），相关研究陷入停滞或缓慢发展的阶段。



Q3: 如何在“精确检索”与“上下文丰富性”之间权衡？
传统的切片方式在“切分大小”上往往顾此失彼：切小了上下文缺失，切大了检索不准
Sentence Window Retrieval 很好地解决了这个矛盾：
1. 它利用单句进行向量化和检索，确保了检索的高精确度（Small-to-Retrieve）。
2. 它利用窗口元数据在检索后动态扩展上下文，确保了喂给大模型的信息具有足够的丰富性（Large-to-Generate）。

4. 结论
*总结哪种策略在本次实验中表现最好，以及在实际应用中该如何选择。*
根据设置不同参数和切片策略的结果，我觉得# --- 实验 3: Sentence Window Retrieval ---的效果最好
它成功结合了 Sentence Splitter 的检索精准度和 Token Splitter 无法提供的长上下文优势。通过后处理（Post-processing）机制，它不仅准确找到了答案所在的句子，还提供了完整的背景信息，使得大模型生成的回答最为详尽和准确。
建议：在实际生产环境中，如果文档逻辑性强且用户问题需要依赖上下文理解（如法律文档、技术手册），优先推荐使用句子窗口检索策略。